{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup expects samples to be in the ``data_training`` directory in the following structure (``*_mmap`` directories with ``RaggedMmap`` objects storing computed features)\n",
    "\n",
    "```\n",
    "data_training >\n",
    "    testing >\n",
    "        unknown >\n",
    "            negative_batch_1_mmap\n",
    "        wakeword >\n",
    "            positive_batch_1_mmap\n",
    "    training >\n",
    "        unknown >\n",
    "            negative_train_1_mmap\n",
    "        wakeword >\n",
    "            positive_train_1_mmap\n",
    "    validation >\n",
    "        unknown >\n",
    "            negative_validation_1_mmap\n",
    "        wakeword >\n",
    "            positive_validation_1_mmap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'trained_models/inception'\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1490\n",
    "FEATURE_BIN_COUNT = 40\n",
    "TIME_SHIFT_MS = 0.0\n",
    "WINDOW_STRIDE = 20\n",
    "WINDOW_SIZE_MS = 30\n",
    "PREPROCESS = 'none'\n",
    "WANTED_WORDS = \"wakeword,unknown\"\n",
    "DATASET_DIR =  'data_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m kws_streaming.train.model_train_eval \\\n",
    "--data_url='' \\\n",
    "--data_dir={DATASET_DIR} \\\n",
    "--train_dir={TRAIN_DIR} \\\n",
    "--split_data 0 \\\n",
    "--mel_upper_edge_hertz 7500.0 \\\n",
    "--mel_lower_edge_hertz 125.0 \\\n",
    "--how_many_training_steps 10000 \\\n",
    "--learning_rate 0.001 \\\n",
    "--window_size_ms={WINDOW_SIZE_MS} \\\n",
    "--window_stride_ms={WINDOW_STRIDE} \\\n",
    "--clip_duration_ms={CLIP_DURATION_MS} \\\n",
    "--eval_step_interval=500 \\\n",
    "--mel_num_bins={FEATURE_BIN_COUNT} \\\n",
    "--dct_num_features={FEATURE_BIN_COUNT} \\\n",
    "--preprocess={PREPROCESS} \\\n",
    "--alsologtostderr \\\n",
    "--train 1 \\\n",
    "--wanted_words={WANTED_WORDS} \\\n",
    "--pick_deterministically 0 \\\n",
    "--return_softmax 1 \\\n",
    "--restore_checkpoint 0 \\\n",
    "inception \\\n",
    "--cnn1_filters '32' \\\n",
    "--cnn1_kernel_sizes '5' \\\n",
    "--cnn1_strides '1' \\\n",
    "--cnn2_filters1 '16,16,16' \\\n",
    "--cnn2_filters2 '32,64,70' \\\n",
    "--cnn2_kernel_sizes '3,5,5' \\\n",
    "--cnn2_strides '1,1,1' \\\n",
    "--dropout 0.0 \\\n",
    "--bn_scale 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the saved internal streaming model to a quantified tflite model\n",
    "# requires a representative dataset for the generated features in the file representative_dataset.txt\n",
    "# Must be run after restarting the interpreter, as it needs to clear the old sessions and run in eager mode\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "features = np.load('hey_jarvis_live_features.npy') # features generated from a short sample with the wake word said once\n",
    "\n",
    "features[0,0] = 26.0  # guarantee one pixel is the preprocessor max\n",
    "\n",
    "def representative_dataset_gen():\n",
    "  for i in range(features.shape[0]):\n",
    "    yield [features[i,:]]\n",
    "\n",
    "model_path = os.path.join('trained_models', 'inception','stream_state_internal')\n",
    "converter = tf.compat.v2.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "\n",
    "converter.experimental_new_quantizer = True\n",
    "converter.experimental_enable_resource_variables = True\n",
    "converter.experimental_new_converter = True\n",
    "converter._experimental_variable_quantization = True\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.inference_type=tf.int8\n",
    "converter.inference_input_type=tf.int8\n",
    "converter.inference_output_type=tf.uint8\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "saved_tflite_path = os.path.join('trained_models', 'inception', 'stream_state_internal_quantized.tflite')\n",
    "tflite_model_size = open(saved_tflite_path, \"wb\").write(tflite_model)\n",
    "print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_tflite_path = os.path.join('trained_models', 'inception', 'stream_state_internal_quantized.tflite')\n",
    "tflite_flat_buffer_path = os.path.join('trained_models', 'inception', 'stream_state_internal_quantized.cc')\n",
    "\n",
    "!xxd -i {saved_tflite_path} \\\n",
    "> {tflite_flat_buffer_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
